{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACXF Demo: Adaptive Context-Aware Explanation Generation\n",
        "\n",
        "This notebook demonstrates the ACXF system for generating adaptive explanations for tabular data classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import importlib\n",
        "\n",
        "# Add parent directory to path so src can be imported as a package\n",
        "sys.path.insert(0, str(Path('..')))\n",
        "\n",
        "from src.utils.loaders import load_dataset\n",
        "from src.utils.preprocessing import preprocess_data\n",
        "from src.models.train_models import train_model\n",
        "from src.profiling.user_profiler import UserProfiler, UserCategory\n",
        "from src.explanation.context_engine import ContextAwareEngine, DecisionCriticality, TimePressure\n",
        "from src.consistency.consistency_tracker import ConsistencyTracker\n",
        "from src.interface.novice_view import NoviceView\n",
        "from src.interface.intermediate_view import IntermediateView\n",
        "from src.interface.expert_view import ExpertView\n",
        "\n",
        "# Force reload modules to ensure latest code is used (do this after imports)\n",
        "if 'src.explanation.lime_explainer' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.explanation.lime_explainer'])\n",
        "if 'src.explanation.context_engine' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.explanation.context_engine'])\n",
        "    # Re-import after reload\n",
        "    from src.explanation.context_engine import ContextAwareEngine, DecisionCriticality, TimePressure\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Preprocess Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using synthetic dataset\n",
            "Dataset shape: (1000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Try to load a dataset\n",
        "data_dir = Path('..') / 'data'\n",
        "datasets = {\n",
        "    'german_credit': data_dir / 'german_credit.csv',\n",
        "    'diabetes': data_dir / 'diabetes.csv',\n",
        "    'telco_churn': data_dir / 'telco_churn.csv'\n",
        "}\n",
        "\n",
        "# Find available dataset\n",
        "dataset_name = None\n",
        "dataset_path = None\n",
        "\n",
        "for name, path in datasets.items():\n",
        "    if path.exists():\n",
        "        dataset_name = name\n",
        "        dataset_path = path\n",
        "        break\n",
        "\n",
        "if dataset_path is None:\n",
        "    # Create synthetic dataset\n",
        "    from sklearn.datasets import make_classification\n",
        "    X, y = make_classification(\n",
        "        n_samples=1000, n_features=10, n_informative=5,\n",
        "        n_redundant=2, n_classes=2, random_state=42\n",
        "    )\n",
        "    feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    y_series = pd.Series(y, name='target')\n",
        "    print(\"Using synthetic dataset\")\n",
        "else:\n",
        "    print(f\"Loading dataset: {dataset_name}\")\n",
        "    X_df, y_series, feature_names = load_dataset(dataset_name, str(dataset_path))\n",
        "\n",
        "print(f\"Dataset shape: {X_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed shape: (1000, 10)\n",
            "Number of features: 10\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "X_processed, y_processed, preprocess_info = preprocess_data(X_df, y_series)\n",
        "feature_names_processed = preprocess_info['feature_names']\n",
        "print(f\"Preprocessed shape: {X_processed.shape}\")\n",
        "print(f\"Number of features: {len(feature_names_processed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained successfully!\n",
            "Train Accuracy: 1.0000\n",
            "Test Accuracy: 0.9350\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "model, X_train, X_test, y_train, y_test, metrics = train_model(\n",
        "    X_processed, y_processed, model_type='random_forest', test_size=0.2\n",
        ")\n",
        "\n",
        "print(f\"Model trained successfully!\")\n",
        "print(f\"Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
        "print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize ACXF Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ACXF components initialized!\n"
          ]
        }
      ],
      "source": [
        "# Initialize consistency tracker\n",
        "consistency_tracker = ConsistencyTracker(n_clusters=10)\n",
        "consistency_tracker.fit_clusters(X_train)\n",
        "\n",
        "# Create user personas\n",
        "novice_profiler = UserProfiler(UserCategory.NOVICE)\n",
        "novice_profiler.update_from_questionnaire(1, 2, 2, 2)\n",
        "\n",
        "intermediate_profiler = UserProfiler(UserCategory.INTERMEDIATE)\n",
        "intermediate_profiler.update_from_questionnaire(3, 3, 3, 3)\n",
        "\n",
        "expert_profiler = UserProfiler(UserCategory.EXPERT)\n",
        "expert_profiler.update_from_questionnaire(5, 5, 4, 5)\n",
        "\n",
        "# Create views\n",
        "novice_view = NoviceView()\n",
        "intermediate_view = IntermediateView()\n",
        "expert_view = ExpertView()\n",
        "\n",
        "print(\"ACXF components initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Adaptive Explanations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Instance #0\n",
            "True Label: 1, Predicted: 1\n",
            "Prediction Probabilities: [0.04354062 0.95645938]\n"
          ]
        }
      ],
      "source": [
        "# Select a test instance\n",
        "test_idx = 0\n",
        "test_instance = X_test[test_idx]\n",
        "true_label = y_test[test_idx]\n",
        "prediction = model.predict_proba(test_instance.reshape(1, -1))[0]\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "print(f\"Test Instance #{test_idx}\")\n",
        "print(f\"True Label: {true_label}, Predicted: {predicted_class}\")\n",
        "print(f\"Prediction Probabilities: {prediction}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Novice User Explanation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOVICE USER EXPLANATION\n",
            "======================================================================\n",
            "The model's decision is primarily influenced by: Feature_4, Feature_5, Feature_0.\n"
          ]
        }
      ],
      "source": [
        "# Create context engine for novice\n",
        "novice_engine = ContextAwareEngine(\n",
        "    model, X_train, feature_names_processed, novice_profiler\n",
        ")\n",
        "\n",
        "# Generate explanation\n",
        "novice_explanation = novice_engine.generate_explanation(\n",
        "    test_instance,\n",
        "    decision_criticality=DecisionCriticality.MEDIUM,\n",
        "    time_pressure=TimePressure.NONE\n",
        ")\n",
        "\n",
        "# Render\n",
        "print(\"NOVICE USER EXPLANATION\")\n",
        "print(\"=\" * 70)\n",
        "print(novice_view.render(novice_explanation))\n",
        "\n",
        "# Visualize\n",
        "Path('../experiments/plots').mkdir(parents=True, exist_ok=True)\n",
        "novice_view.visualize(novice_explanation, save_path='../experiments/plots/novice_explanation.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Intermediate User Explanation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INTERMEDIATE USER EXPLANATION\n",
            "======================================================================\n",
            "Feature Contribution Analysis:\n",
            "\n",
            "Method: LIME\n",
            "\n",
            " 1. Feature_4                      ↓ 0.1427\n",
            " 2. Feature_5                      ↓ 0.0906\n",
            " 3. Feature_0                      ↑ 0.0843\n",
            " 4. Feature_3                      ↑ 0.0592\n",
            " 5. Feature_9                      ↑ 0.0514\n",
            " 6. Feature_6                      ↓ 0.0059\n",
            " 7. Feature_7                      ↓ 0.0033\n",
            " 8. Feature_1                      ↓ 0.0029\n",
            " 9. Feature_8                      ↓ 0.0014\n",
            "10. Feature_2                      ↑ 0.0009\n",
            "\n",
            "Predicted probabilities:\n",
            "  Class 0: 0.044\n",
            "  Class 1: 0.956\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create context engine for intermediate\n",
        "intermediate_engine = ContextAwareEngine(\n",
        "    model, X_train, feature_names_processed, intermediate_profiler\n",
        ")\n",
        "\n",
        "# Generate explanation\n",
        "intermediate_explanation = intermediate_engine.generate_explanation(\n",
        "    test_instance,\n",
        "    decision_criticality=DecisionCriticality.MEDIUM,\n",
        "    time_pressure=TimePressure.NONE\n",
        ")\n",
        "\n",
        "# Render\n",
        "print(\"INTERMEDIATE USER EXPLANATION\")\n",
        "print(\"=\" * 70)\n",
        "print(intermediate_view.render(intermediate_explanation))\n",
        "\n",
        "# Visualize\n",
        "intermediate_view.visualize(intermediate_explanation, save_path='../experiments/plots/intermediate_explanation.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Expert User Explanation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXPERT USER EXPLANATION\n",
            "======================================================================\n",
            "Detailed Explanation Report\n",
            "==================================================\n",
            "\n",
            "Method: SHAP (Local)\n",
            "Type: local\n",
            "\n",
            "Feature Contributions:\n",
            "--------------------------------------------------\n",
            "  1. Feature_9                             0.074056\n",
            "  2. Feature_8                            -0.074056\n",
            "  3. Feature_3                             0.058442\n",
            "  4. Feature_2                            -0.058442\n",
            "  5. Feature_1                             0.052908\n",
            "  6. Feature_0                            -0.052908\n",
            "  7. Feature_7                            -0.021684\n",
            "  8. Feature_6                             0.021684\n",
            "  9. Feature_4                            -0.002275\n",
            " 10. Feature_5                             0.002275\n",
            "\n",
            "Base Value (Expected): 0.501037\n",
            "\n",
            "Predicted Probabilities:\n",
            "  Class 0: 0.043541\n",
            "  Class 1: 0.956459\n",
            "  Predicted Class: 1\n",
            "\n",
            "Instance Values:\n",
            "  Feature_0: 0.0700\n",
            "  Feature_1: -1.9764\n",
            "  Feature_2: 0.5167\n",
            "  Feature_3: -0.4833\n",
            "  Feature_4: -0.6989\n",
            "  Feature_5: -0.8536\n",
            "  Feature_6: -1.3213\n",
            "  Feature_7: 0.5003\n",
            "  Feature_8: -0.2716\n",
            "  Feature_9: 1.3502\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create context engine for expert\n",
        "expert_engine = ContextAwareEngine(\n",
        "    model, X_train, feature_names_processed, expert_profiler\n",
        ")\n",
        "\n",
        "# Generate explanation\n",
        "expert_explanation = expert_engine.generate_explanation(\n",
        "    test_instance,\n",
        "    decision_criticality=DecisionCriticality.HIGH,\n",
        "    time_pressure=TimePressure.NONE\n",
        ")\n",
        "\n",
        "# Render\n",
        "print(\"EXPERT USER EXPLANATION\")\n",
        "print(\"=\" * 70)\n",
        "print(expert_view.render(expert_explanation))\n",
        "\n",
        "# Visualize\n",
        "expert_view.visualize_shap_bar(expert_explanation, save_path='../experiments/plots/expert_explanation.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Counterfactual Explanations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COUNTERFACTUAL EXPLANATION\n",
            "======================================================================\n",
            "Original Prediction: [0.0435406162464986, 0.9564593837535013]\n",
            "Counterfactual Prediction: [0.1531111111111111, 0.8468888888888888]\n",
            "Class Changed: False\n",
            "\n",
            "Modifications:\n",
            "  Feature_4: -0.699 -> -1.048\n",
            "  Feature_5: -0.854 -> -1.280\n",
            "  Feature_0: 0.070 -> 0.035\n",
            "  Feature_3: -0.483 -> -0.242\n",
            "  Feature_9: 1.350 -> 0.675\n"
          ]
        }
      ],
      "source": [
        "# Generate counterfactual\n",
        "counterfactual = expert_engine.generate_counterfactual(test_instance)\n",
        "\n",
        "print(\"COUNTERFACTUAL EXPLANATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Original Prediction: {counterfactual.get('original_prediction', [])}\")\n",
        "print(f\"Counterfactual Prediction: {counterfactual.get('counterfactual_prediction', [])}\")\n",
        "print(f\"Class Changed: {counterfactual.get('class_changed', False)}\")\n",
        "\n",
        "if counterfactual.get('modifications'):\n",
        "    print(\"\\nModifications:\")\n",
        "    for mod in counterfactual['modifications'][:5]:\n",
        "        print(f\"  {mod.get('feature', 'Unknown')}: \"\n",
        "              f\"{mod.get('original_value', 0):.3f} -> {mod.get('new_value', 0):.3f}\")\n",
        "\n",
        "# Visualize\n",
        "expert_view.visualize_counterfactual(counterfactual, save_path='../experiments/plots/counterfactual.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Temporal Consistency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Consistency Score: 0.000\n",
            "Consistency Scores: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# Test consistency across multiple instances\n",
        "test_indices = [0, 1, 2, 3, 4]\n",
        "consistency_scores = []\n",
        "\n",
        "for idx in test_indices:\n",
        "    instance = X_test[idx]\n",
        "    explanation = expert_engine.generate_explanation(instance)\n",
        "    \n",
        "    consistency = consistency_tracker.compute_consistency_score(instance, explanation)\n",
        "    consistency_scores.append(consistency.get('consistency_score', 0.0))\n",
        "    \n",
        "    consistency_tracker.add_explanation(instance, explanation)\n",
        "\n",
        "print(f\"Average Consistency Score: {np.mean(consistency_scores):.3f}\")\n",
        "print(f\"Consistency Scores: {consistency_scores}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "This demo showed:\n",
        "1. How ACXF adapts explanations based on user expertise level\n",
        "2. Different explanation formats for novice, intermediate, and expert users\n",
        "3. Counterfactual explanation generation\n",
        "4. Temporal consistency tracking\n",
        "\n",
        "For full evaluation, run the `evaluation_results.ipynb` notebook.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
